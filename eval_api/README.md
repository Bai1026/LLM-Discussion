# Evaluation API Tool
Evaluate Results using **Fluency**, **Flexibility**, **Originality**, and **Elaboration**
## Setup and Installation

1. **Clone the Repository:**
   Clone or download this repository to your local machine.

2. **Install Dependencies:**
   Navigate to the project directory and run:
   ```bash
   pip install -r requirement.txt
   ```
3. Before you execute, create a file name `.env` in the same folder. It must contain:
   ```bash
   api_key="YOUR_API_KEY"
   ```

### 1. Setting File Paths:
For `auto_grade.py`, you don't have to edit path
  ```bash
  Put your input json file into "dataset" folder
  ```
Output for `auto_grade.py` will be stored in `/result` folder <br /><br />

For `box_plot.py`, set the file paths for the data you want to analyze:
  ```bash
  Choose 2 Files from "/result" Folder
  filename1  = "your_first_filename"
  filename2 = "your_second_filename"
  # e.g. filename = "file1"
  ```
Output for `box_plot.py` will be stored in `analysis_img/boxplot` folder <br /><br />

For `mean_std.py`, set the file path to the dataset you want to analyze:
  ```bash
  Choose 1 File from "/result" Folder
  filename = "your_filename"
  # e.g. filename = "file2"
  ```
### 2. Running the Script
  To run `auto_grade.py`:
  ```bash
  python3 auto_grade.py --version {gpt version} --input_file {input file name} --sample {# of samples}
  ```


-----


## Folders

#### `result/`

This folder contains the output JSON files generated by the `auto_grade.py` script. These files are related to an automated grading process where various aspects of responses are evaluated, such as fluency, flexibility, originality, and elaboration. Each file in this directory represents the output from a single run of the script.

#### `analysis_img/`

This directory stores PNG files that are the output of the `box_plot.py` script. These images are box plot visualizations generated from data analysis, comparing different aspects like fluency, flexibility, originality, and elaboration.

## Python Scripts

#### `auto_grade.py`

This script is an automated grading tool that evaluates responses based on fluency, flexibility, originality, and elaboration. It uses OpenAI's API to generate responses and assess them based on the specified criteria. The script handles data loading and caching, error handling, and output formatting. The results are saved as JSON files in the `result` directory. Key features include:

- Loading and caching responses to improve efficiency.
- Parsing and evaluating responses based on a set of criteria.
- Generating detailed evaluation reports in JSON format.

#### `box_plot.py`

This script generates box plot visualizations for the evaluation of responses. It analyzes the JSON output files from `auto_grade.py` and calculates mean and standard deviation for different evaluation criteria (fluency, flexibility, originality, and elaboration). The script then creates box plots to visualize the distribution of these scores, allowing for easy comparison of different datasets or models. Key features include:

- Calculating statistical measures (mean and standard deviation).
- Generating box plots for different evaluation categories.
- Saving the plots as PNG files in the `analysis_img` directory.

#### `mean_std.py`

This script calculates the mean and standard deviation for the evaluation categories (fluency, flexibility, originality, and elaboration) based on the data in a specified JSON file. It's useful for getting a quick statistical overview of the evaluation results. Key features include:

- Reading data from a JSON file.
- Computing mean and standard deviation for different evaluation categories.
- Displaying calculated values in the console for quick analysis.

